# -*- coding: utf-8 -*-
"""nbeatsx_full_features.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P9BXq6jl8dx5HiXTbwYAm3fudkt5KkFI
"""

import os
from google.colab import drive
drive.mount('/content/drive')
sample_data_path = '/content/drive/MyDrive/Colab Notebooks'
files=os.listdir(sample_data_path)
print(files)
ff='/content/drive/MyDrive/Colab Notebooks/featured_shihara.xlsx'
import pandas as pd
df=pd.read_excel(ff,engine='openpyxl')
print(df.head())
print(df.dtypes)
df["Date"]=pd.to_datetime(df["Date"])
nf_df=df

# Required libraries
!pip install neuralforecast
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from neuralforecast import NeuralForecast
from neuralforecast.models import NBEATSx
from neuralforecast.losses import MAE, MSE
from neuralforecast.utils import AirPassengersDF
from sklearn.metrics import mean_absolute_error, mean_squared_error
import itertools

print(nf_df.head())

# Time-based features
nf_df['day_of_month'] = nf_df['Date'].dt.day
nf_df['month'] = nf_df['Date'].dt.month
nf_df['year'] = nf_df['Date'].dt.year
nf_df['quarter'] = nf_df['Date'].dt.quarter
nf_df['is_weekend'] = nf_df['Date'].dt.dayofweek >= 5
nf_df['is_month_start'] = nf_df['Date'].dt.is_month_start.astype(int)
nf_df['is_month_end'] = nf_df['Date'].dt.is_month_end.astype(int)

# Advanced rolling features with min_periods to handle NaN values
for window in [7, 14, 30]:
    nf_df[f'rolling_mean_{window}d'] = nf_df['Normalized_Balance'].rolling(window=window, min_periods=1).mean()
    nf_df[f'rolling_std_{window}d'] = nf_df['Normalized_Balance'].rolling(window=window, min_periods=1).std()
    nf_df[f'rolling_min_{window}d'] = nf_df['Normalized_Balance'].rolling(window=window, min_periods=1).min()
    nf_df[f'rolling_max_{window}d'] = nf_df['Normalized_Balance'].rolling(window=window, min_periods=1).max()
    nf_df[f'rolling_skew_{window}d'] = nf_df['Normalized_Balance'].rolling(window=window, min_periods=1).skew()
    nf_df[f'rolling_kurt_{window}d'] = nf_df['Normalized_Balance'].rolling(window=window, min_periods=1).kurt()

# Momentum and rate of change features
nf_df['balance_diff_1d'] = nf_df['Normalized_Balance'].diff(1)
nf_df['balance_diff_7d'] = nf_df['Normalized_Balance'].diff(7)
nf_df['balance_pct_change_1d'] = nf_df['Normalized_Balance'].pct_change(1).fillna(0).clip(-1, 1)
nf_df['balance_pct_change_7d'] = nf_df['Normalized_Balance'].pct_change(7).fillna(0).clip(-1, 1)

# Exponential weighted features
nf_df['ewm_7d'] = nf_df['Normalized_Balance'].ewm(span=7, min_periods=1).mean()
nf_df['ewm_14d'] = nf_df['Normalized_Balance'].ewm(span=14, min_periods=1).mean()
nf_df['ewm_30d'] = nf_df['Normalized_Balance'].ewm(span=30, min_periods=1).mean()

# Fill NaN values
nf_df = nf_df.fillna(method='bfill').fillna(method='ffill')

# Data preparation: Split into train (90%) and test (10%)
train_size = int(len(nf_df) * 0.9)
train_df = nf_df.iloc[:train_size].copy()
test_df = nf_df.iloc[train_size:].copy()

# Handle missing values in both datasets
train_df = train_df.fillna(method='bfill').fillna(method='ffill')
test_df = test_df.fillna(method='bfill').fillna(method='ffill')

# Set up the data in NeuralForecast format
train_data = train_df.copy()
train_data['unique_id'] = 'balance'
train_data = train_data.rename(columns={'Date': 'ds', 'Normalized_Balance': 'y'})

test_data = test_df.copy()
test_data['unique_id'] = 'balance'
test_data = test_data.rename(columns={'Date': 'ds', 'Normalized_Balance': 'y'})

# Define exogenous variables
exogenous_vars = [col for col in train_data.columns
                  if col not in ['ds', 'y', 'unique_id']]

# Define hyperparameter grid
param_grid = {
    'input_size': [160],  # Best performing input size from previous runs
    'max_steps': [1000],  # Using 1000 epochs as requested
    'learning_rate': [0.001],  # Best performing learning rate
    'batch_size': [1]  # Using batch size of 1 for maximum accuracy (stochastic gradient descent)
}

# Initialize results storage
results = []

# Perform grid search
for input_size, max_steps, lr, batch_size in itertools.product(
    param_grid['input_size'],
    param_grid['max_steps'],
    param_grid['learning_rate'],
    param_grid['batch_size']
):
    print(f"\nTesting parameters: input_size={input_size}, max_steps={max_steps}, lr={lr}, batch_size={batch_size}")

    model = NBEATSx(
        h=30,  # Prediction horizon for next 30 days
        input_size=input_size,
        max_steps=max_steps,
        learning_rate=lr,
        batch_size=batch_size,
        futr_exog_list=exogenous_vars,
        hist_exog_list=exogenous_vars,
        random_seed=42,
        scaler_type='standard',
        loss=MAE(),  # Using MAE loss for better robustness
        valid_loss=MAE(),  # Using MAE for validation
        num_stacks=10,  # Increasing number of stacks for better feature extraction
        n_blocks=3,  # Increasing number of blocks for better pattern recognition
        mlp_units=[[256, 256, 256, 256]],  # Increasing model capacity with multiple layers
        dropout_prob_theta=0.1,  # Adding dropout for better generalization
        activation='ReLU',  # Using ReLU activation
        shared_weights=False,  # Not sharing weights for better feature learning
        stack_types=['identity', 'trend', 'seasonality']  # Using all stack types for better pattern recognition
    )
    
    forecaster = NeuralForecast(
        models=[model],
        freq='D'
    )

    # Fit the model
    forecaster.fit(df=train_data)

    # Generate forecasts
    forecast_df = forecaster.predict(futr_df=test_data.iloc[:30])

    # Calculate metrics
    actual = test_data['y'].iloc[:30].values
    forecast = forecast_df.loc[forecast_df['unique_id'] == 'balance', 'NBEATSx'].values
    
    mae = mean_absolute_error(actual, forecast)
    rmse = np.sqrt(mean_squared_error(actual, forecast))
    
    results.append({
        'input_size': input_size,
        'max_steps': max_steps,
        'learning_rate': lr,
        'batch_size': batch_size,
        'mae': mae,
        'rmse': rmse,
        'forecast': forecast
    })
    
    print(f"MAE: {mae:.4f}, RMSE: {rmse:.4f}")

# Convert results to DataFrame
results_df = pd.DataFrame(results)

# Find best parameters based on MAE and RMSE
best_mae_idx = results_df['mae'].idxmin()
best_rmse_idx = results_df['rmse'].idxmin()

print("\nBest parameters based on MAE:")
print(results_df.iloc[best_mae_idx])
print("\nBest parameters based on RMSE:")
print(results_df.iloc[best_rmse_idx])

# Plot results
plt.figure(figsize=(15, 10))

# Plot 1: Actual vs Best MAE Forecast
plt.subplot(2, 1, 1)
plt.plot(range(30), actual, label='Actual', marker='o', alpha=0.7)
plt.plot(range(30), results_df.iloc[best_mae_idx]['forecast'], 
         label=f'Best MAE Forecast (MAE: {results_df.iloc[best_mae_idx]["mae"]:.4f})', 
         marker='x', alpha=0.7)
plt.title('Best MAE Forecast vs Actual')
plt.xlabel('Days')
plt.ylabel('Normalized Balance')
plt.legend()
plt.grid(True)

# Plot 2: Actual vs Best RMSE Forecast
plt.subplot(2, 1, 2)
plt.plot(range(30), actual, label='Actual', marker='o', alpha=0.7)
plt.plot(range(30), results_df.iloc[best_rmse_idx]['forecast'], 
         label=f'Best RMSE Forecast (RMSE: {results_df.iloc[best_rmse_idx]["rmse"]:.4f})', 
         marker='x', alpha=0.7)
plt.title('Best RMSE Forecast vs Actual')
plt.xlabel('Days')
plt.ylabel('Normalized Balance')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Plot parameter impact on errors
plt.figure(figsize=(15, 10))

# Plot 1: Input Size vs Errors
plt.subplot(2, 2, 1)
for input_size in param_grid['input_size']:
    mask = results_df['input_size'] == input_size
    plt.scatter(results_df[mask]['mae'], results_df[mask]['rmse'], 
               label=f'input_size={input_size}', alpha=0.7)
plt.xlabel('MAE')
plt.ylabel('RMSE')
plt.title('Input Size Impact')
plt.legend()
plt.grid(True)

# Plot 2: Max Steps vs Errors
plt.subplot(2, 2, 2)
for max_steps in param_grid['max_steps']:
    mask = results_df['max_steps'] == max_steps
    plt.scatter(results_df[mask]['mae'], results_df[mask]['rmse'], 
               label=f'max_steps={max_steps}', alpha=0.7)
plt.xlabel('MAE')
plt.ylabel('RMSE')
plt.title('Max Steps Impact')
plt.legend()
plt.grid(True)

# Plot 3: Learning Rate vs Errors
plt.subplot(2, 2, 3)
for lr in param_grid['learning_rate']:
    mask = results_df['learning_rate'] == lr
    plt.scatter(results_df[mask]['mae'], results_df[mask]['rmse'], 
               label=f'lr={lr}', alpha=0.7)
plt.xlabel('MAE')
plt.ylabel('RMSE')
plt.title('Learning Rate Impact')
plt.legend()
plt.grid(True)

# Plot 4: Batch Size vs Errors
plt.subplot(2, 2, 4)
for batch_size in param_grid['batch_size']:
    mask = results_df['batch_size'] == batch_size
    plt.scatter(results_df[mask]['mae'], results_df[mask]['rmse'], 
               label=f'batch_size={batch_size}', alpha=0.7)
plt.xlabel('MAE')
plt.ylabel('RMSE')
plt.title('Batch Size Impact')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

