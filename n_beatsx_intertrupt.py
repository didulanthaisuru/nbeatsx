# -*- coding: utf-8 -*-
"""n_beatsx_version_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1481LAB5izbFs0zcjA2qADVt-Nslr2dPe
"""

import os
from google.colab import drive
drive.mount('/content/drive')
sample_data_path = '/content/drive/MyDrive/Colab Notebooks'
files=os.listdir(sample_data_path)
print(files)
ff='/content/drive/MyDrive/Colab Notebooks/featured_shihara.xlsx'
import pandas as pd
df=pd.read_excel(ff,engine='openpyxl')
print(df.head())
print(df.dtypes)
df["Date"]=pd.to_datetime(df["Date"])
nf_df=df

# Required libraries
import subprocess
subprocess.check_call(['pip', 'install', 'neuralforecast'])
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from neuralforecast import NeuralForecast
from neuralforecast.models import NBEATSx
from neuralforecast.utils import AirPassengersDF
from sklearn.metrics import mean_absolute_error, mean_squared_error
from statsmodels.tsa.seasonal import seasonal_decompose
import seaborn as sns
from tabulate import tabulate
from neuralforecast.conformal import ConformalPredictionIntervals
from neuralforecast.losses import MAE
import torch
import torch.nn as nn
from torchmetrics import MeanAbsoluteError

# Custom MAE loss using PyTorch
class PyTorchMAE(nn.Module):
    def __init__(self):
        super().__init__()
        self.mae = MeanAbsoluteError()
    
    def forward(self, y_pred, y_true):
        return self.mae(y_pred, y_true)

# Data preparation: Split into train (90%) and test (10%)
train_size = int(len(nf_df) * 0.9)
train_df = nf_df.iloc[:train_size].copy()
test_df = nf_df.iloc[train_size:].copy()

print(f"Training set size: {len(train_df)}")
print(f"Testing set size: {len(test_df)}")

# Handle missing values in both datasets
train_df = train_df.fillna(method='bfill').fillna(method='ffill')
test_df = test_df.fillna(method='bfill').fillna(method='ffill')

# Set up the data in NeuralForecast format
train_data = train_df.copy()
train_data['unique_id'] = 'balance'  # Add required ID column
train_data = train_data.rename(columns={'Date': 'ds', 'Normalized_Balance': 'y'})

test_data = test_df.copy()
test_data['unique_id'] = 'balance'
test_data = test_data.rename(columns={'Date': 'ds', 'Normalized_Balance': 'y'})

# Define exogenous variables (all features except date, target and ID)
exogenous_vars = [col for col in train_data.columns
                  if col not in ['ds', 'y', 'unique_id']]

print(f"Using {len(exogenous_vars)} exogenous variables: {exogenous_vars}")

# Define forecast horizon
horizon = 30

# Create multiple models with different random seeds for ensemble
models = [
    NBEATSx(
        h=horizon,
        input_size=160,
        futr_exog_list=exogenous_vars,
        hist_exog_list=exogenous_vars,
        random_seed=seed,
        scaler_type='standard',
        loss=PyTorchMAE()  # Using PyTorch MAE
    ) for seed in [42, 43, 44, 45, 46]  # Using 5 different seeds for ensemble
]

# Create the forecaster with multiple models
forecaster = NeuralForecast(
    models=models,
    freq='D'
)

# Fit the model with prediction intervals
forecaster.fit(
    df=train_data,
    prediction_intervals=ConformalPredictionIntervals(
        n_windows=5,
        h=horizon
    )
)

# Generate forecasts with prediction intervals
forecast_df = forecaster.predict(
    futr_df=test_data.iloc[:horizon],
    level=[80, 95]  # 80% and 95% prediction intervals
)

# Extract actual values and forecasts
actual = test_data['y'].iloc[:horizon].values
forecast = forecast_df.loc[forecast_df['unique_id'] == 'balance', 'NBEATSx'].values

# Calculate error metrics using PyTorch
actual_tensor = torch.tensor(actual, dtype=torch.float32)
forecast_tensor = torch.tensor(forecast, dtype=torch.float32)
pytorch_mae = PyTorchMAE()(forecast_tensor, actual_tensor).item()

# Calculate other metrics
mae = mean_absolute_error(actual, forecast)
rmse = np.sqrt(mean_squared_error(actual, forecast))
mape = np.mean(np.abs((actual - forecast) / (actual + 1e-8))) * 100

print("\nError Metrics (PyTorch Implementation):")
print(f"PyTorch MAE: {pytorch_mae:.4f}")
print(f"Scikit-learn MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAPE: {mape:.2f}%")

# Calculate uncertainty metrics
def calculate_uncertainty_metrics(actual, forecast, lower_80, upper_80, lower_95, upper_95):
    # Calculate prediction intervals coverage
    coverage_80 = np.mean((actual >= lower_80) & (actual <= upper_80)) * 100
    coverage_95 = np.mean((actual >= lower_95) & (actual <= upper_95)) * 100
    
    # Calculate prediction interval width
    width_80 = np.mean(upper_80 - lower_80)
    width_95 = np.mean(upper_95 - lower_95)
    
    # Calculate confidence score (inverse of prediction interval width)
    confidence_score = 1 / (1 + width_80)
    
    return coverage_80, coverage_95, width_80, width_95, confidence_score

# Get prediction intervals
lower_80 = forecast_df.loc[forecast_df['unique_id'] == 'balance', 'NBEATSx-lo-80'].values
upper_80 = forecast_df.loc[forecast_df['unique_id'] == 'balance', 'NBEATSx-hi-80'].values
lower_95 = forecast_df.loc[forecast_df['unique_id'] == 'balance', 'NBEATSx-lo-95'].values
upper_95 = forecast_df.loc[forecast_df['unique_id'] == 'balance', 'NBEATSx-hi-95'].values

# Calculate uncertainty metrics
coverage_80, coverage_95, width_80, width_95, confidence_score = calculate_uncertainty_metrics(
    actual, forecast, lower_80, upper_80, lower_95, upper_95
)

# Plot results with prediction intervals
plt.figure(figsize=(15, 8))
plt.plot(range(horizon), actual, label='Actual', marker='o', alpha=0.7)
plt.plot(range(horizon), forecast, label='NBEATSx Forecast', marker='x', alpha=0.7)
plt.fill_between(range(horizon), lower_95, upper_95, color='gray', alpha=0.2, label='95% Prediction Interval')
plt.fill_between(range(horizon), lower_80, upper_80, color='gray', alpha=0.3, label='80% Prediction Interval')
plt.title('30-Day Balance Forecast with Uncertainty Intervals')
plt.xlabel('Days')
plt.ylabel('Normalized Balance')
plt.legend()
plt.grid(True)
plt.tight_layout()

print("\nPrediction Performance Metrics:")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")

print("\nUncertainty Metrics:")
print(f"80% Prediction Interval Coverage: {coverage_80:.2f}%")
print(f"95% Prediction Interval Coverage: {coverage_95:.2f}%")
print(f"Average 80% Prediction Interval Width: {width_80:.4f}")
print(f"Average 95% Prediction Interval Width: {width_95:.4f}")
print(f"Average Confidence Score: {confidence_score:.4f}")

# Calculate and display prediction confidence for each day
daily_confidence = 1 / (1 + (upper_80 - lower_80))
print("\nDaily Prediction Confidence:")
for day in range(horizon):
    print(f"Day {day+1}: {daily_confidence[day]:.2%} confidence")

plt.show()

# Create detailed results table
from statsmodels.tsa.seasonal import seasonal_decompose
import seaborn as sns
from tabulate import tabulate

# Perform seasonal decomposition
decomposition = seasonal_decompose(train_data['y'], period=30)  # Assuming monthly seasonality
trend = decomposition.trend
seasonal = decomposition.seasonal

# Calculate numerical trend and seasonality metrics
def calculate_trend_metrics(values, window=5):
    if len(values) < window:
        return 0, "Insufficient data"
    
    recent_trend = values[-window:].mean() - values[-window-1:-1].mean()
    trend_percentage = (recent_trend / values[-window-1:-1].mean()) * 100
    
    if abs(trend_percentage) < 0.1:
        return trend_percentage, "Stable"
    return trend_percentage, "Increasing" if trend_percentage > 0 else "Decreasing"

def calculate_seasonality_metrics(seasonal_values, original_values):
    strength = np.std(seasonal_values) / np.std(original_values)
    amplitude = np.max(seasonal_values) - np.min(seasonal_values)
    return strength, amplitude

# Prepare table data
table_data = []
for i in range(horizon):
    date = test_data['ds'].iloc[i]
    actual_val = actual[i]
    pred_val = forecast[i]
    uncertainty = (upper_80[i] - lower_80[i]) / 2  # Half-width of 80% prediction interval
    confidence = daily_confidence[i]
    
    # Get numerical trend and seasonality metrics
    trend_percentage, trend_direction = calculate_trend_metrics(train_data['y'].values)
    seasonality_strength, seasonality_amplitude = calculate_seasonality_metrics(seasonal.values, train_data['y'].values)
    
    # Create detailed explanation with numerical values
    explanation = (
        f"Trend: {trend_direction} ({trend_percentage:.2f}%), "
        f"Seasonality: Strength={seasonality_strength:.3f}, "
        f"Amplitude={seasonality_amplitude:.3f}"
    )
    
    table_data.append([
        date.strftime('%Y-%m-%d'),
        f"{pred_val:.4f}",
        f"{actual_val:.4f}",
        f"{uncertainty:.4f}",
        f"{confidence:.2%}",
        explanation
    ])

# Create and display the table
headers = ['Date', 'Prediction', 'Actual', 'Uncertainty', 'Confidence', 'Analysis']
print("\nDetailed Forecast Results:")
print(tabulate(table_data, headers=headers, tablefmt='grid'))

# Create a heatmap of prediction confidence
plt.figure(figsize=(12, 6))
confidence_matrix = daily_confidence.reshape(1, -1)
sns.heatmap(confidence_matrix, 
            cmap='RdYlGn',
            xticklabels=range(1, horizon + 1),
            yticklabels=['Confidence'],
            cbar_kws={'label': 'Confidence Score'})
plt.title('Prediction Confidence Heatmap')
plt.xlabel('Forecast Day')
plt.tight_layout()
plt.show()

# Plot trend and seasonality components with numerical values
plt.figure(figsize=(15, 10))
plt.subplot(3, 1, 1)
plt.plot(train_data['ds'], train_data['y'], label='Original')
plt.plot(train_data['ds'], trend, label=f'Trend ({trend_percentage:.2f}%)')
plt.title('Trend Analysis')
plt.legend()

plt.subplot(3, 1, 2)
plt.plot(train_data['ds'], seasonal, 
         label=f'Seasonal (Strength={seasonality_strength:.3f}, Amplitude={seasonality_amplitude:.3f})')
plt.title('Seasonality Analysis')
plt.legend()

plt.subplot(3, 1, 3)
residual = train_data['y'] - trend - seasonal
plt.plot(train_data['ds'], residual, 
         label=f'Residual (Std={np.std(residual):.3f})')
plt.title('Residual Analysis')
plt.legend()

plt.tight_layout()
plt.show()

# Save results to CSV with additional metrics
results_df = pd.DataFrame(table_data, columns=headers)
results_df['Trend_Percentage'] = trend_percentage
results_df['Seasonality_Strength'] = seasonality_strength
results_df['Seasonality_Amplitude'] = seasonality_amplitude
results_df.to_csv('forecast_results.csv', index=False)
print("\nResults have been saved to 'forecast_results.csv'")

# Feature importance analysis
def calculate_feature_importance(model, train_data, exogenous_vars):
    importance_scores = {}
    baseline_forecast = model.predict(train_data)
    baseline_error = mean_squared_error(train_data['y'], baseline_forecast['NBEATSx'])
    
    for feature in exogenous_vars:
        # Create a copy of the data with shuffled feature
        shuffled_data = train_data.copy()
        shuffled_data[feature] = np.random.permutation(shuffled_data[feature])
        
        # Get new forecast with shuffled feature
        shuffled_forecast = model.predict(shuffled_data)
        shuffled_error = mean_squared_error(shuffled_data['y'], shuffled_forecast['NBEATSx'])
        
        # Calculate importance as increase in error
        importance = (shuffled_error - baseline_error) / baseline_error
        importance_scores[feature] = importance
    
    return importance_scores

# Calculate and display feature importance
print("\nFeature Importance Analysis:")
feature_importance = calculate_feature_importance(models[0], train_data, exogenous_vars)
for feature, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):
    print(f"{feature}: {importance:.4f}")

