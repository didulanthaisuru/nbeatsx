# -*- coding: utf-8 -*-
"""n_beatsx_version_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1481LAB5izbFs0zcjA2qADVt-Nslr2dPe
"""

import os
from google.colab import drive
drive.mount('/content/drive')
sample_data_path = '/content/drive/MyDrive/Colab Notebooks'
files=os.listdir(sample_data_path)
print(files)
ff='/content/drive/MyDrive/Colab Notebooks/featured_shihara.xlsx'
import pandas as pd
df=pd.read_excel(ff,engine='openpyxl')
print(df.head())
print(df.dtypes)
df["Date"]=pd.to_datetime(df["Date"])
nf_df=df

# Required libraries
!pip install neuralforecast
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from neuralforecast import NeuralForecast
from neuralforecast.models import NBEATSx
from neuralforecast.utils import AirPassengersDF
from sklearn.metrics import mean_absolute_error, mean_squared_error
from itertools import product

# Data preparation: Split into train (90%) and test (10%)
train_size = int(len(nf_df) * 0.9)
train_df = nf_df.iloc[:train_size].copy()
test_df = nf_df.iloc[train_size:].copy()

print(f"Training set size: {len(train_df)}")
print(f"Testing set size: {len(test_df)}")

# Handle missing values in both datasets
train_df = train_df.fillna(method='bfill').fillna(method='ffill')
test_df = test_df.fillna(method='bfill').fillna(method='ffill')

# Set up the data in NeuralForecast format
train_data = train_df.copy()
train_data['unique_id'] = 'balance'  # Add required ID column
train_data = train_data.rename(columns={'Date': 'ds', 'Normalized_Balance': 'y'})

test_data = test_df.copy()
test_data['unique_id'] = 'balance'
test_data = test_data.rename(columns={'Date': 'ds', 'Normalized_Balance': 'y'})

# Define exogenous variables (all features except date, target and ID)
exogenous_vars = [col for col in train_data.columns
                  if col not in ['ds', 'y', 'unique_id']]

print(f"Using {len(exogenous_vars)} exogenous variables: {exogenous_vars}")

# Define forecast horizon
horizon = 30

# Define hyperparameter grid
param_grid = {
    'input_size': [120, 160, 200],
    'max_steps': [1000, 2000, 3000],
    'learning_rate': [0.001, 0.0005, 0.0001],
    'batch_size': [32, 64, 128],
    'n_harmonics': [1, 2, 3],
    'n_polynomials': [1, 2, 3]
}

# Function to evaluate model with given parameters
def evaluate_model(params):
    model = NBEATSx(
        h=horizon,
        input_size=params['input_size'],
        max_steps=params['max_steps'],
        learning_rate=params['learning_rate'],
        batch_size=params['batch_size'],
        n_harmonics=params['n_harmonics'],
        n_polynomials=params['n_polynomials'],
        futr_exog_list=exogenous_vars,
        hist_exog_list=exogenous_vars,
        random_seed=42,
        scaler_type='standard'
    )
    
    forecaster = NeuralForecast(
        models=[model],
        freq='D'
    )
    
    forecaster.fit(df=train_data)
    forecast_df = forecaster.predict(futr_df=test_data.iloc[:horizon])
    
    actual = test_data['y'].iloc[:horizon].values
    forecast = forecast_df.loc[forecast_df['unique_id'] == 'balance', 'NBEATSx'].values
    
    mae = mean_absolute_error(actual, forecast)
    rmse = np.sqrt(mean_squared_error(actual, forecast))
    mape = np.mean(np.abs((actual - forecast) / (actual + 1e-8))) * 100
    
    return {'mae': mae, 'rmse': rmse, 'mape': mape, 'params': params}

# Perform grid search
best_mae = float('inf')
best_params = None
best_metrics = None

# Create parameter combinations
param_combinations = [dict(zip(param_grid.keys(), v)) for v in product(*param_grid.values())]

print("Starting hyperparameter tuning...")
for i, params in enumerate(param_combinations):
    print(f"\nTrying combination {i+1}/{len(param_combinations)}")
    print(f"Parameters: {params}")
    
    metrics = evaluate_model(params)
    print(f"Metrics - MAE: {metrics['mae']:.4f}, RMSE: {metrics['rmse']:.4f}, MAPE: {metrics['mape']:.2f}%")
    
    if metrics['mae'] < best_mae:
        best_mae = metrics['mae']
        best_params = params
        best_metrics = metrics
        print("New best parameters found!")

print("\nBest parameters found:")
print(best_params)
print("\nBest metrics:")
print(f"MAE: {best_metrics['mae']:.4f}")
print(f"RMSE: {best_metrics['rmse']:.4f}")
print(f"MAPE: {best_metrics['mape']:.2f}%")

# Train final model with best parameters
final_model = NBEATSx(
    h=horizon,
    input_size=best_params['input_size'],
    max_steps=best_params['max_steps'],
    learning_rate=best_params['learning_rate'],
    batch_size=best_params['batch_size'],
    n_harmonics=best_params['n_harmonics'],
    n_polynomials=best_params['n_polynomials'],
    futr_exog_list=exogenous_vars,
    hist_exog_list=exogenous_vars,
    random_seed=42,
    scaler_type='standard'
)

final_forecaster = NeuralForecast(
    models=[final_model],
    freq='D'
)

final_forecaster.fit(df=train_data)
final_forecast_df = final_forecaster.predict(futr_df=test_data.iloc[:horizon])

# Plot final results
plt.figure(figsize=(12, 6))
plt.plot(range(horizon), test_data['y'].iloc[:horizon].values, label='Actual', marker='o', alpha=0.7)
plt.plot(range(horizon), final_forecast_df.loc[final_forecast_df['unique_id'] == 'balance', 'NBEATSx'].values, 
         label='NBEATSx Forecast', marker='x', alpha=0.7)
plt.title('30-Day Balance Forecast vs Actual (Optimized Model)')
plt.xlabel('Days')
plt.ylabel('Normalized Balance')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

